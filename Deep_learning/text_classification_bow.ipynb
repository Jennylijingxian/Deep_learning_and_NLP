{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:51:18.088250Z",
     "start_time": "2019-05-23T04:51:17.426066Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of word model for a text classification problem. Note that this is not the same as the continous bag of word problem that we solved here but you can reuse the tokenization part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:51:21.661953Z",
     "start_time": "2019-05-23T04:51:21.658143Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    ! wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
    "    ! mkdir data\n",
    "    ! tar -xvf rotten_imdb.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:51:22.437635Z",
     "start_time": "2019-05-23T04:51:22.287902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-20 21:55:05--  http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
      "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.20\n",
      "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.20|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 519599 (507K) [application/x-gzip]\n",
      "Saving to: ‘rotten_imdb.tar.gz’\n",
      "\n",
      "rotten_imdb.tar.gz  100%[===================>] 507.42K  2.29MB/s    in 0.2s    \n",
      "\n",
      "2020-05-20 21:55:05 (2.29 MB/s) - ‘rotten_imdb.tar.gz’ saved [519599/519599]\n",
      "\n",
      "x quote.tok.gt9.5000\n",
      "x plot.tok.gt9.5000\n",
      "x subjdata.README.1.0\n",
      "plot.tok.gt9.5000   quote.tok.gt9.5000  subjdata.README.1.0\n"
     ]
    }
   ],
   "source": [
    "get_data()\n",
    "! ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\" Read file returns a list of lines.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:51:31.979291Z",
     "start_time": "2019-05-23T04:51:30.952129Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_content = read_file(\"data/quote.tok.gt9.5000\")\n",
    "obj_content = read_file(\"data/plot.tok.gt9.5000\")\n",
    "sub_content = np.array([line.strip().lower() for line in sub_content])\n",
    "obj_content = np.array([line.strip().lower() for line in obj_content])\n",
    "sub_y = np.zeros(len(sub_content))\n",
    "obj_y = np.ones(len(obj_content))\n",
    "X = np.append(sub_content, obj_content)\n",
    "y = np.append(sub_y, obj_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:51:31.992431Z",
     "start_time": "2019-05-23T04:51:31.982777Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000,), (8000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"both lead performances are oscar-size . quaid is utterly fearless as the tortured husband living a painful lie , and moore wonderfully underplays the long-suffering heroine with an unflappable '50s dignity somewhere between jane wyman and june cleaver .\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a vocabulary\n",
    "* Split your sentences in tokens by spliting on spaces.\n",
    "* Compute the frequency of every word.\n",
    "* Pick top frequency words (4000 or so) to be part of your vocabulary.\n",
    "* Create a map from each word to an index. Keep 0 for out of the vocabulary workds (<UNK>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_token = [s.strip().lower().split(\" \")for s in x_train]\n",
    "x_val_token = [s.strip().lower().split(\" \")for s in x_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_token = []\n",
    "for s in x_train_token:\n",
    "    total_token += s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_4000 = list(list(zip(*c.most_common(4000)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = dict(zip(['UNK']+most_frequent_4000,range(4001))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dict(zip(range(4001),zip(['UNK']+most_frequent_4000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of word representation\n",
    "\n",
    "* Given a piece of text compute the following features $x$.\n",
    "$x_i = 1$ if word with index $i$ appears in the text. Otherwise $x_i = 0$. Note that length $x$ is the size of the vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_representation(sentence):\n",
    "    idx = np.zeros(4001)\n",
    "    for w in sentence:\n",
    "        try:\n",
    "            tmp = vocab2index[w]\n",
    "            idx[tmp]=1\n",
    "        except KeyError:\n",
    "            continue \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = [np.array(bow_representation(s)) for s in x_train_token]\n",
    "idx_val = [np.array(bow_representation(s)) for s in x_val_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset and dataloaders\n",
    "Write a dataset for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:52:54.922573Z",
     "start_time": "2019-05-23T04:52:54.916113Z"
    }
   },
   "outputs": [],
   "source": [
    "class BOW(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx],self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = BOW(idx_train,y_train)\n",
    "val = BOW(idx_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train,batch_size=400,shuffle=True)\n",
    "valid_dl = DataLoader(val,batch_size=400,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 1.,  ..., 0., 0., 0.]], dtype=torch.float64),\n",
       " tensor([1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "         1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "         0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "         0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "         0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
       "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
       "         1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "         1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "         0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         0., 0., 1., 0.]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Define a two layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWModel(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super(BOWModel,self).__init__()\n",
    "        self.linear1 = nn.Linear(vocab_size,40)\n",
    "        self.linear2 = nn.Linear(40,1)\n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and valid functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:56:29.431871Z",
     "start_time": "2019-05-23T04:56:29.426088Z"
    }
   },
   "outputs": [],
   "source": [
    "def val_metrics(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0\n",
    "    for x, y in valid_dl:\n",
    "        y_hat = model(x.float())\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y.unsqueeze(1).float())\n",
    "        y_pred = y_hat > 0\n",
    "        correct += (y_pred.float() == y.unsqueeze(1).float()).float().sum()\n",
    "        total += x.size(0)\n",
    "        loss_sum += loss.item()*x.size(0)\n",
    "    accuracy = correct.item()/total\n",
    "    return loss_sum/total, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:04:48.131508Z",
     "start_time": "2019-05-22T09:04:48.124575Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        loss_sum = 0\n",
    "        for x, y in train_dl:\n",
    "            y_hat = model(x.float())\n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat, y.unsqueeze(1).float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += x.size(0)\n",
    "            loss_sum += loss.item()*x.size(0)\n",
    "        val_loss, val_acc = val_metrics(model)\n",
    "        print(\"train loss %.3f val loss %.3f and accuracy %.3f\" % (loss_sum/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:05:03.287290Z",
     "start_time": "2019-05-22T09:04:54.494282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.659 val loss 0.610 and accuracy 0.891\n",
      "train loss 0.547 val loss 0.498 and accuracy 0.899\n",
      "train loss 0.429 val loss 0.399 and accuracy 0.898\n",
      "train loss 0.335 val loss 0.331 and accuracy 0.902\n",
      "train loss 0.272 val loss 0.290 and accuracy 0.904\n",
      "train loss 0.230 val loss 0.264 and accuracy 0.903\n",
      "train loss 0.200 val loss 0.248 and accuracy 0.905\n",
      "train loss 0.178 val loss 0.238 and accuracy 0.908\n",
      "train loss 0.160 val loss 0.231 and accuracy 0.911\n",
      "train loss 0.145 val loss 0.227 and accuracy 0.908\n"
     ]
    }
   ],
   "source": [
    "model = BOWModel(vocab_size=len(vocab2index.keys()))\n",
    "train_epocs(model, 10, 0.0009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word importance\n",
    "To get the words that affect the most the positive label we find the words with higher weights. Similarly to get the words that affect the most the 0 label we find the words with lower weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0129,  0.0951,  0.0503,  ..., -0.0044, -0.0179, -0.0038],\n",
       "         [-0.0029,  0.0752,  0.0356,  ..., -0.0339, -0.0180, -0.0075],\n",
       "         [ 0.0031,  0.0676,  0.0920,  ...,  0.0369,  0.0267,  0.0283],\n",
       "         ...,\n",
       "         [-0.0057,  0.0660,  0.0389,  ..., -0.0053, -0.0258, -0.0053],\n",
       "         [-0.0153,  0.0677,  0.0809,  ...,  0.0186,  0.0128,  0.0175],\n",
       "         [ 0.0109,  0.0935,  0.0578,  ..., -0.0321, -0.0090, -0.0102]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0883, 0.0754, 0.0776, 0.0916, 0.0894, 0.0784, 0.0749, 0.1157, 0.0759,\n",
       "         0.0874, 0.0791, 0.0885, 0.0761, 0.1153, 0.0837, 0.0785, 0.0945, 0.0624,\n",
       "         0.0935, 0.0750, 0.1017, 0.0824, 0.1026, 0.0797, 0.0641, 0.1132, 0.0816,\n",
       "         0.0993, 0.0606, 0.0807, 0.0874, 0.1056, 0.0762, 0.0950, 0.0803, 0.0737,\n",
       "         0.0569, 0.0703, 0.0736, 0.0964], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1883, -0.2634,  0.2803, -0.1743,  0.3046,  0.1939, -0.2491,  0.1665,\n",
       "          -0.2620, -0.2716,  0.2238,  0.2789,  0.2069, -0.1669, -0.3076, -0.1902,\n",
       "           0.2122, -0.2401,  0.1993,  0.2915,  0.1783, -0.3040, -0.1677, -0.2745,\n",
       "          -0.2702,  0.1789,  0.2935,  0.1894, -0.3126,  0.2423, -0.1951, -0.1746,\n",
       "           0.2395, -0.1683,  0.2088, -0.3040, -0.2646, -0.2542,  0.2559, -0.1737]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0505], requires_grad=True)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parms = [p for p in model.parameters()]\n",
    "parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01290899,  0.09514178,  0.05026132, ..., -0.00436886,\n",
       "        -0.01786191, -0.00381802],\n",
       "       [-0.00292029,  0.07519574,  0.03555987, ..., -0.03394768,\n",
       "        -0.01798485, -0.00745508],\n",
       "       [ 0.00309361,  0.06760807,  0.09202307, ...,  0.03694866,\n",
       "         0.02668696,  0.02834515],\n",
       "       ...,\n",
       "       [-0.00571659,  0.06595954,  0.03887386, ..., -0.00534975,\n",
       "        -0.02584901, -0.00534588],\n",
       "       [-0.01533698,  0.06765414,  0.08089469, ...,  0.01859844,\n",
       "         0.01279914,  0.01754384],\n",
       "       [ 0.01094467,  0.09346816,  0.05777293, ..., -0.03209402,\n",
       "        -0.00898605, -0.01020165]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = parms[0].detach().numpy()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4001,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indeces = np.argsort(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.1477776, 0.148554)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0, sorted_indeces[0]], weights[0, sorted_indeces[-1]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they',),\n",
       " ('he',),\n",
       " ('discovers',),\n",
       " ('friends',),\n",
       " ('-',),\n",
       " ('kill',),\n",
       " ('follows',),\n",
       " ('her',),\n",
       " ('she',),\n",
       " ('struggles',)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in sorted_indeces[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('screen',),\n",
       " ('here',),\n",
       " ('actors',),\n",
       " ('good',),\n",
       " ('me',),\n",
       " ('likely',),\n",
       " ('performance',),\n",
       " ('interesting',),\n",
       " ('my',),\n",
       " ('--',),\n",
       " ('material',)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in sorted_indeces[3990:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
