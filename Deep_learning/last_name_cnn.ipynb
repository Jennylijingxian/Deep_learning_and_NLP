{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifing last names with character-level CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "`https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_train.csv.gz`\n",
    "\n",
    "`https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_test.csv.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz \n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz \n",
    "    ! mkdir -p data\n",
    "    ! gunzip names_train.csv.gz \n",
    "    ! gunzip names_test.csv.gz\n",
    "    ! mv names*.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-01 14:45:49--  https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.76.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.76.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50237 (49K) [application/octet-stream]\n",
      "Saving to: ‘names_train.csv.gz’\n",
      "\n",
      "names_train.csv.gz  100%[===================>]  49.06K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2020-06-01 14:45:49 (2.27 MB/s) - ‘names_train.csv.gz’ saved [50237/50237]\n",
      "\n",
      "--2020-06-01 14:45:49--  https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.76.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.76.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27541 (27K) [application/octet-stream]\n",
      "Saving to: ‘names_test.csv.gz’\n",
      "\n",
      "names_test.csv.gz   100%[===================>]  26.90K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2020-06-01 14:45:49 (2.30 MB/s) - ‘names_test.csv.gz’ saved [27541/27541]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/582_fri_c1_500_25.tsv.gz'),\n",
       " PosixPath('data/519_vinnie.tsv.gz'),\n",
       " PosixPath('data/618_fri_c3_1000_50.tsv.gz'),\n",
       " PosixPath('data/626_fri_c2_500_50.tsv.gz'),\n",
       " PosixPath('data/678_visualizing_environmental.tsv.gz'),\n",
       " PosixPath('data/574_house_16H.tsv.gz'),\n",
       " PosixPath('data/598_fri_c0_1000_25.tsv.gz'),\n",
       " PosixPath('data/631_fri_c1_500_5.tsv.gz'),\n",
       " PosixPath('data/584_fri_c4_500_25.tsv.gz'),\n",
       " PosixPath('data/599_fri_c2_1000_5.tsv.gz'),\n",
       " PosixPath('data/643_fri_c2_500_25.tsv.gz'),\n",
       " PosixPath('data/579_fri_c0_250_5.tsv.gz'),\n",
       " PosixPath('data/647_fri_c1_250_10.tsv.gz'),\n",
       " PosixPath('data/690_visualizing_galaxy.tsv.gz'),\n",
       " PosixPath('data/1196_BNG_pharynx.tsv.gz'),\n",
       " PosixPath('data/593_fri_c1_1000_10.tsv.gz'),\n",
       " PosixPath('data/633_fri_c0_500_25.tsv.gz'),\n",
       " PosixPath('data/581_fri_c3_500_25.tsv.gz'),\n",
       " PosixPath('data/.DS_Store'),\n",
       " PosixPath('data/228_elusage.tsv.gz'),\n",
       " PosixPath('data/225_puma8NH.tsv.gz'),\n",
       " PosixPath('data/641_fri_c1_500_10.tsv.gz'),\n",
       " PosixPath('data/218_house_8L.tsv.gz'),\n",
       " PosixPath('data/560_bodyfat.tsv.gz'),\n",
       " PosixPath('data/628_fri_c3_1000_5.tsv.gz'),\n",
       " PosixPath('data/1193_BNG_lowbwt.tsv.gz'),\n",
       " PosixPath('data/621_fri_c0_100_10.tsv.gz'),\n",
       " PosixPath('data/485_analcatdata_vehicle.tsv.gz'),\n",
       " PosixPath('data/344_mv.tsv.gz'),\n",
       " PosixPath('data/620_fri_c1_1000_25.tsv.gz'),\n",
       " PosixPath('data/603_fri_c0_250_50.tsv.gz'),\n",
       " PosixPath('data/505_tecator.tsv.gz'),\n",
       " PosixPath('data/names_train.csv'),\n",
       " PosixPath('data/622_fri_c2_1000_50.tsv.gz'),\n",
       " PosixPath('data/656_fri_c1_100_5.tsv.gz'),\n",
       " PosixPath('data/591_fri_c1_100_10.tsv.gz'),\n",
       " PosixPath('data/612_fri_c1_1000_5.tsv.gz'),\n",
       " PosixPath('data/706_sleuth_case1202.tsv.gz'),\n",
       " PosixPath('data/597_fri_c2_500_5.tsv.gz'),\n",
       " PosixPath('data/586_fri_c3_1000_25.tsv.gz'),\n",
       " PosixPath('data/623_fri_c4_1000_10.tsv.gz'),\n",
       " PosixPath('data/646_fri_c3_500_10.tsv.gz'),\n",
       " PosixPath('data/names_test.csv'),\n",
       " PosixPath('data/654_fri_c0_500_10.tsv.gz'),\n",
       " PosixPath('data/562_cpu_small.tsv.gz'),\n",
       " PosixPath('data/1089_USCrime.tsv.gz'),\n",
       " PosixPath('data/645_fri_c3_500_50.tsv.gz'),\n",
       " PosixPath('data/617_fri_c3_500_5.tsv.gz'),\n",
       " PosixPath('data/595_fri_c0_1000_10.tsv.gz'),\n",
       " PosixPath('data/561_cpu.tsv.gz'),\n",
       " PosixPath('data/616_fri_c4_500_50.tsv.gz'),\n",
       " PosixPath('data/542_pollution.tsv.gz'),\n",
       " PosixPath('data/663_rabe_266.tsv.gz'),\n",
       " PosixPath('data/712_chscase_geyser1.tsv.gz'),\n",
       " PosixPath('data/522_pm10.tsv.gz'),\n",
       " PosixPath('data/590_fri_c0_1000_50.tsv.gz'),\n",
       " PosixPath('data/601_fri_c1_250_5.tsv.gz'),\n",
       " PosixPath('data/models'),\n",
       " PosixPath('data/634_fri_c2_100_10.tsv.gz'),\n",
       " PosixPath('data/605_fri_c2_250_25.tsv.gz'),\n",
       " PosixPath('data/592_fri_c4_1000_25.tsv.gz'),\n",
       " PosixPath('data/503_wind.tsv.gz'),\n",
       " PosixPath('data/192_vineyard.tsv.gz'),\n",
       " PosixPath('data/651_fri_c0_100_25.tsv.gz'),\n",
       " PosixPath('data/609_fri_c0_1000_5.tsv.gz'),\n",
       " PosixPath('data/227_cpu_small.tsv.gz'),\n",
       " PosixPath('data/197_cpu_act.tsv.gz'),\n",
       " PosixPath('data/665_sleuth_case2002.tsv.gz'),\n",
       " PosixPath('data/695_chatfield_4.tsv.gz'),\n",
       " PosixPath('data/588_fri_c4_1000_100.tsv.gz'),\n",
       " PosixPath('data/589_fri_c2_1000_25.tsv.gz'),\n",
       " PosixPath('data/627_fri_c2_500_10.tsv.gz'),\n",
       " PosixPath('data/615_fri_c4_250_10.tsv.gz'),\n",
       " PosixPath('data/583_fri_c1_1000_50.tsv.gz'),\n",
       " PosixPath('data/557_analcatdata_apnea1.tsv.gz'),\n",
       " PosixPath('data/604_fri_c4_500_10.tsv.gz'),\n",
       " PosixPath('data/1199_BNG_echoMonths.tsv.gz'),\n",
       " PosixPath('data/602_fri_c3_250_10.tsv.gz'),\n",
       " PosixPath('data/1029_LEV.tsv.gz'),\n",
       " PosixPath('data/653_fri_c0_250_25.tsv.gz'),\n",
       " PosixPath('data/607_fri_c4_1000_50.tsv.gz'),\n",
       " PosixPath('data/666_rmftsa_ladata.tsv.gz'),\n",
       " PosixPath('data/687_sleuth_ex1605.tsv.gz'),\n",
       " PosixPath('data/573_cpu_act.tsv.gz'),\n",
       " PosixPath('data/644_fri_c4_250_25.tsv.gz'),\n",
       " PosixPath('data/210_cloud.tsv.gz'),\n",
       " PosixPath('data/1027_ESL.tsv.gz'),\n",
       " PosixPath('data/659_sleuth_ex1714.tsv.gz'),\n",
       " PosixPath('data/201_pol.tsv.gz'),\n",
       " PosixPath('data/635_fri_c0_250_10.tsv.gz'),\n",
       " PosixPath('data/230_machine_cpu.tsv.gz'),\n",
       " PosixPath('data/657_fri_c2_250_10.tsv.gz'),\n",
       " PosixPath('data/4544_GeographicalOriginalofMusic.tsv.gz'),\n",
       " PosixPath('data/608_fri_c3_1000_10.tsv.gz'),\n",
       " PosixPath('data/527_analcatdata_election2000.tsv.gz'),\n",
       " PosixPath('data/294_satellite_image.tsv.gz'),\n",
       " PosixPath('data/195_auto_price.tsv.gz'),\n",
       " PosixPath('data/556_analcatdata_apnea2.tsv.gz'),\n",
       " PosixPath('data/594_fri_c2_100_5.tsv.gz'),\n",
       " PosixPath('data/547_no2.tsv.gz'),\n",
       " PosixPath('data/529_pollen.tsv.gz'),\n",
       " PosixPath('data/1030_ERA.tsv.gz'),\n",
       " PosixPath('data/649_fri_c0_500_5.tsv.gz'),\n",
       " PosixPath('data/648_fri_c1_250_50.tsv.gz'),\n",
       " PosixPath('data/611_fri_c3_100_5.tsv.gz'),\n",
       " PosixPath('data/650_fri_c0_500_50.tsv.gz'),\n",
       " PosixPath('data/1028_SWD.tsv.gz'),\n",
       " PosixPath('data/564_fried.tsv.gz'),\n",
       " PosixPath('data/613_fri_c3_250_5.tsv.gz'),\n",
       " PosixPath('data/624_fri_c0_100_5.tsv.gz'),\n",
       " PosixPath('data/1203_BNG_pwLinear.tsv.gz'),\n",
       " PosixPath('data/1191_BNG_pbc.tsv.gz'),\n",
       " PosixPath('data/229_pwLinear.tsv.gz'),\n",
       " PosixPath('data/596_fri_c2_250_5.tsv.gz'),\n",
       " PosixPath('data/1096_FacultySalaries.tsv.gz'),\n",
       " PosixPath('data/523_analcatdata_neavote.tsv.gz'),\n",
       " PosixPath('data/658_fri_c3_250_25.tsv.gz'),\n",
       " PosixPath('data/537_houses.tsv.gz'),\n",
       " PosixPath('data/606_fri_c2_1000_10.tsv.gz'),\n",
       " PosixPath('data/1595_poker.tsv.gz'),\n",
       " PosixPath('data/1201_BNG_breastTumor.tsv.gz'),\n",
       " PosixPath('data/637_fri_c1_500_50.tsv.gz'),\n",
       " PosixPath('data/207_autoPrice.tsv.gz'),\n",
       " PosixPath('data/215_2dplanes.tsv.gz')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"data\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Adsit\",\"Czech\"\r",
      "\r\n",
      "\"Ajdrna\",\"Czech\"\r",
      "\r\n",
      "\"Antonowitsch\",\"Czech\"\r",
      "\r\n",
      "\"Antonowitz\",\"Czech\"\r",
      "\r\n",
      "\"Ballalatak\",\"Czech\"\r",
      "\r\n",
      "\"Ballaltick\",\"Czech\"\r",
      "\r\n",
      "\"Bastl\",\"Czech\"\r",
      "\r\n",
      "\"Baroch\",\"Czech\"\r",
      "\r\n",
      "\"Betlach\",\"Czech\"\r",
      "\r\n",
      "\"Biganska\",\"Czech\"\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head data/names_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"names_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Antonowitsch'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', \"'\", ',', 'A', 'B', 'C', 'D', 'E', 'F', 'G']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting a vocabulary of characters\n",
    "letters = [list(l) for l in df[0].values]\n",
    "vocab = sorted(list(set(np.concatenate(np.array(letters)))))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2id = {key:i for i, key in enumerate(vocab)}\n",
    "vocab2id[\" \"] # I am going to use 0 to pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(df[1].unique())\n",
    "label2id = {key:i for i, key in enumerate(labels)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# define example\n",
    "data = [label2id[l] for l in df[1].values]\n",
    "data = array(data)\n",
    "# print(data)\n",
    "# one hot encode\n",
    "encoded = to_categorical(data)\n",
    "print(encoded)\n",
    "# invert encoding\n",
    "inverted = argmax(encoded[0])\n",
    "# print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(x, seq_len=15, vocab2id=vocab2id):\n",
    "    x = list(x)\n",
    "    x = np.array([vocab2id[k] for k in x])\n",
    "    z = np.zeros(seq_len, dtype=np.int32)\n",
    "    n = min(seq_len, x.shape[0])\n",
    "    z[seq_len - n:] = x[0:n]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0, 38, 37, 42, 35, 52, 37, 29, 42,  0, 40, 37],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pad_seq(\"jingxian li\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, path, vocab2id, label2id, seq_len=15, vocab_len=55):\n",
    "        self.df = pd.read_csv(path, header=None)\n",
    "        self.label2id = label2id\n",
    "        self.vocab2id = vocab2id\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_len = vocab_len \n",
    "        self.x = df[0].values\n",
    "        self.y = [self.label2id[l] for l in df[1].values]\n",
    "        self.y = to_categorical(self.y)\n",
    "        self.vocab2id = vocab2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = pad_seq(self.x[idx], self.seq_len, self.vocab2id)\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = NameDataset(PATH/\"names_train.csv\", vocab2id, label2id)\n",
    "val = NameDataset(PATH/\"names_test.csv\", vocab2id, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "n=len(val)\n",
    "train_dl = DataLoader(train, batch_size=batch_size,shuffle=True)\n",
    "val_dl = DataLoader(val, batch_size=n,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13374, 13374)"
      ]
     },
     "execution_count": 914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,) [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x,y = train[0]\n",
    "print(x.shape,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3, 32, 47, 37, 48],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, V, D):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(V, D, padding_idx=0)\n",
    "\n",
    "        self.conv_3 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=3)\n",
    "        self.conv_4 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=4)\n",
    "        self.conv_5 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(200, 18)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x3 = F.relu(self.conv_3(x))\n",
    "        x4 = F.relu(self.conv_4(x))\n",
    "        x5 = F.relu(self.conv_5(x))\n",
    "        x3 = nn.MaxPool1d(kernel_size = 6)(x3)\n",
    "        x4 = nn.MaxPool1d(kernel_size = 7)(x4)\n",
    "        x5 = nn.MaxPool1d(kernel_size = 8)(x5)\n",
    "        out = torch.cat([x3, x4, x5], 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dl = DataLoader(train, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 30\n",
    "N = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(V, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 15]),\n",
       " tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(tr_dl))\n",
    "x.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0, 10, 29, 46, 42, 33, 47, 47],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0, 22, 29, 48, 47, 49, 53, 29],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 12, 37, 46, 42, 43, 50],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0, 27, 43, 41, 39, 37, 42, 47],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 13, 46, 49, 47, 33],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4, 29, 36, 29, 46],\n",
       "        [ 0,  0,  0,  0,  0,  0, 18, 29, 37, 32, 53, 47, 36, 33, 50],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 16, 43, 30, 30, 47],\n",
       "        [ 0,  0,  0,  0,  0,  0, 15, 43, 54, 36, 29, 37, 47, 39, 53],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0, 12, 37, 35, 35, 33, 42, 47]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = emb(x.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15, 30])"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 30, 15])"
      ]
     },
     "execution_count": 925,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x1.transpose(1,2)  # needs to convert x to (batch, embedding_dim, sentence_len)\n",
    "x1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_3 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = conv_3(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 13])"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_4 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=4)\n",
    "conv_5 = nn.Conv1d(in_channels=D, out_channels=50, kernel_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50, 12]) torch.Size([10, 50, 11])\n"
     ]
    }
   ],
   "source": [
    "x4 = conv_4(x1)\n",
    "x5 = conv_5(x1)\n",
    "print(x4.size(), x5.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 2])"
      ]
     },
     "execution_count": 931,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 3-gram detectors\n",
    "x3 = nn.ReLU()(x3)\n",
    "x3 = nn.MaxPool1d(kernel_size = 6)(x3)\n",
    "x3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 1])"
      ]
     },
     "execution_count": 932,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 4-gram detectors\n",
    "x4 = nn.ReLU()(x4)\n",
    "x4 = nn.MaxPool1d(kernel_size = 7)(x4)\n",
    "x4.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 1])"
      ]
     },
     "execution_count": 933,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 5-gram detectors\n",
    "x5 = nn.ReLU()(x5)\n",
    "x5 = nn.MaxPool1d(kernel_size = 8)(x5)\n",
    "x5.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 4])"
      ]
     },
     "execution_count": 934,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate x3, x4, x5\n",
    "out = torch.cat([x3, x4, x5], 2)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 200])"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.view(out.size(0), -1)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_metrics(model):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x, y in val_dl:\n",
    "        x = x.long()  #.cuda()\n",
    "        y = y.float()\n",
    "        batch = y.shape[0]\n",
    "        out = model(x)\n",
    "        out_max = np.argmax(out.detach().numpy(),axis = 1)\n",
    "        y_max = np.argmax(y.detach().numpy(),axis = 1)\n",
    "        tmp = (y_max==out_max).astype(int)\n",
    "        \n",
    "        correct = np.sum(tmp)\n",
    "        total = y.shape[0]\n",
    "        accuracy = correct/total\n",
    "        loss = F.mse_loss(out, y)\n",
    "        sum_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        pred = (out > 0).float()\n",
    "        correct += (pred == y).float().sum().item()\n",
    "    val_loss = sum_loss/total\n",
    "    val_acc = correct/total\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, optimizer, epochs=10):\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        for x, y in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.float()\n",
    "            out = model(x)\n",
    "            loss = F.mse_loss(out,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += x.size(0)*loss.item()\n",
    "            total += x.size(0)\n",
    "        train_loss = total_loss/total\n",
    "        val_loss, val_accuracy = valid_metrics(model)\n",
    "        \n",
    "        print(\"train_loss %.3f val_loss %.3f val_accuracy %.3f\" % (\n",
    "            train_loss, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.01, wd = 0.00001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, lr, train_dl, val_dl, epochs=20):\n",
    "    optim = get_optimizer(model, lr =lr, wd = 0.0)\n",
    "    for i in range(epochs):\n",
    "        loss = train(model, optim, train_dl)\n",
    "        val_loss, val_acc = val_metric(model, val_dl)\n",
    "        if i%5 == 1: print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (loss, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)\n",
    "D = 30\n",
    "N = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.058 val_loss 0.017 val_accuracy 0.570\n",
      "train_loss 0.037 val_loss 0.016 val_accuracy 0.612\n",
      "train_loss 0.033 val_loss 0.015 val_accuracy 0.631\n",
      "train_loss 0.031 val_loss 0.014 val_accuracy 0.649\n",
      "train_loss 0.029 val_loss 0.013 val_accuracy 0.669\n",
      "train_loss 0.028 val_loss 0.013 val_accuracy 0.679\n",
      "train_loss 0.027 val_loss 0.013 val_accuracy 0.688\n",
      "train_loss 0.027 val_loss 0.012 val_accuracy 0.693\n",
      "train_loss 0.026 val_loss 0.012 val_accuracy 0.703\n",
      "train_loss 0.026 val_loss 0.012 val_accuracy 0.712\n"
     ]
    }
   ],
   "source": [
    "model = CNN(V, D)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_epocs(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.026 val_loss 0.012 val_accuracy 0.717\n",
      "train_loss 0.025 val_loss 0.012 val_accuracy 0.724\n",
      "train_loss 0.025 val_loss 0.011 val_accuracy 0.721\n",
      "train_loss 0.025 val_loss 0.011 val_accuracy 0.729\n",
      "train_loss 0.024 val_loss 0.011 val_accuracy 0.730\n",
      "train_loss 0.024 val_loss 0.011 val_accuracy 0.734\n",
      "train_loss 0.024 val_loss 0.011 val_accuracy 0.737\n",
      "train_loss 0.024 val_loss 0.011 val_accuracy 0.742\n",
      "train_loss 0.024 val_loss 0.011 val_accuracy 0.746\n",
      "train_loss 0.023 val_loss 0.011 val_accuracy 0.740\n",
      "train_loss 0.023 val_loss 0.011 val_accuracy 0.745\n",
      "train_loss 0.023 val_loss 0.011 val_accuracy 0.751\n",
      "train_loss 0.023 val_loss 0.011 val_accuracy 0.749\n",
      "train_loss 0.023 val_loss 0.010 val_accuracy 0.750\n",
      "train_loss 0.023 val_loss 0.010 val_accuracy 0.752\n",
      "train_loss 0.023 val_loss 0.010 val_accuracy 0.752\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.756\n",
      "train_loss 0.023 val_loss 0.010 val_accuracy 0.758\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.763\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.763\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.762\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.762\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.766\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.762\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.764\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.767\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.763\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.770\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.770\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.771\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.773\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.770\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.774\n",
      "train_loss 0.022 val_loss 0.010 val_accuracy 0.774\n",
      "train_loss 0.021 val_loss 0.010 val_accuracy 0.773\n",
      "train_loss 0.021 val_loss 0.010 val_accuracy 0.776\n",
      "train_loss 0.021 val_loss 0.010 val_accuracy 0.775\n",
      "train_loss 0.021 val_loss 0.010 val_accuracy 0.777\n",
      "train_loss 0.021 val_loss 0.010 val_accuracy 0.777\n",
      "train_loss 0.021 val_loss 0.009 val_accuracy 0.777\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, optimizer, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
